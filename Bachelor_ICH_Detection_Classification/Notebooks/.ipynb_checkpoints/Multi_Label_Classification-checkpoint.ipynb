{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import precision_score,f1_score\n",
    "import cupy as cp\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random # to set the python random seed\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "#from albumentations import Compose, ShiftScaleRotate, Resize, CenterCrop\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "# Ignore excessive warnings\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# WandB â€“ Import the wandb library\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malqnatri-ahmed\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_mid_slices(LABEL_DIR,EXPORT_DIR):\n",
    "    lables = pd.read_csv(LABEL_DIR)\n",
    "    start, end = 0,0\n",
    "    data = {'Name':[],\n",
    "           'Target':[]} \n",
    "    for i, row in tqdm(lables.iterrows()):\n",
    "        if start == 0:\n",
    "            start = i\n",
    "        if row[\"StudyInstance\"] == lables[\"StudyInstance\"][start]:\n",
    "            #print(f'index {i} file_name {row[\"filename\"]} pataint_name {row[\"PatientID\"]}')\n",
    "            continue\n",
    "        else:\n",
    "            end = i-1\n",
    "            mid = int((end+start)/2)\n",
    "            \n",
    "            data[\"Name\"].append(lables[\"filename\"][mid+2])\n",
    "            data[\"Target\"].append(lables[\"any\"][mid+2])          \n",
    "            \n",
    "            data[\"Name\"].append(lables[\"filename\"][mid])\n",
    "            data[\"Target\"].append(lables[\"any\"][mid])\n",
    "            \n",
    "            data[\"Name\"].append(lables[\"filename\"][mid+1])\n",
    "            data[\"Target\"].append(lables[\"any\"][mid+1])\n",
    "        \n",
    "            data[\"Name\"].append(lables[\"filename\"][mid+3])\n",
    "            data[\"Target\"].append(lables[\"any\"][mid+3])\n",
    "            \n",
    "            data[\"Name\"].append(lables[\"filename\"][mid+4])\n",
    "            data[\"Target\"].append(lables[\"any\"][mid+4])\n",
    "            \n",
    "            start = i\n",
    "    \n",
    "    new_df = pd.DataFrame(data)\n",
    "    new_df.to_csv(EXPORT_DIR,index=False)\n",
    "    \n",
    "    \n",
    "#EXPORT_DIR = 'C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/CSV/test_mid_slice.csv'    \n",
    "#choose_mid_slices('C:/Users/alqna/Documents/Bachelor/Bachelor_Data/csv/train_meta_id_seriser.csv',EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files():\n",
    "    LABEL_DIR = 'C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/CSV/test_subtype_classes_1.csv'\n",
    "    files = pd.read_csv(LABEL_DIR)\n",
    "    src='C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/TEST/'\n",
    "    dst='C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/TEST_MULTI/'\n",
    "\n",
    "    for i, row in tqdm(files.iterrows()):\n",
    "        shutil.copy(src+row['Name'], dst+row['Name'])\n",
    "#move_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(DATA_DIR, TARGETS_FILE, EXPORT_DIR):\n",
    "    p = [f for f in os.listdir(DATA_DIR) if os.path.isfile(os.path.join(DATA_DIR, f))]\n",
    "    df = pd.read_csv(TARGETS_FILE)\n",
    "\n",
    "    data = {'Name':[], \n",
    "            'Target':[]} \n",
    "    counts = [0,0,0,0,0]\n",
    "    flags = [0,0,0,0,0] \n",
    "    for i in tqdm(p):\n",
    "        index = df.index[df['filename'] == i[:-4]]\n",
    "        any_result = df[\"any\"].iloc[index].to_numpy()\n",
    "        if any_result[0] == 0:\n",
    "            continue\n",
    "            #data[\"Target\"].append(any_result[0])\n",
    "            #data[\"Name\"].append(i)\n",
    "        else:\n",
    "            for column in df:\n",
    "                if column == 'any' or column == 'filename':\n",
    "                    continue\n",
    "                else:\n",
    "                    result = df[column].iloc[index].to_numpy()\n",
    "                    if result[0] == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        index_count = df.columns.get_loc(column) - 2\n",
    "                        flags[index_count] = 1\n",
    "            min_index = -1\n",
    "            for flag in range(len(flags)):\n",
    "                if flags[flag] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    if min_index == -1 :\n",
    "                        min_index = flag\n",
    "                    else:\n",
    "                        if counts[flag] < counts[min_index]:\n",
    "                            min_index = flag\n",
    "            counts[min_index] = counts[min_index] + 1\n",
    "                    \n",
    "            data[\"Target\"].append(min_index+1)\n",
    "            data[\"Name\"].append(i)\n",
    "            flags = [0,0,0,0,0]\n",
    "            \n",
    "    new_df = pd.DataFrame(data)\n",
    "    new_df.to_csv(EXPORT_DIR,index=False)\n",
    "\n",
    "    \n",
    "#DATA_DIR='C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/TRAIN/'\n",
    "#TARGETS_FILE='C:/Users/alqna/Documents/Bachelor/Bachelor_Data/csv/standard.csv'\n",
    "#EXPORT_DIR='C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/CSV/train_subtype_classes.csv'\n",
    "#create_targets(DATA_DIR, TARGETS_FILE, EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_lable(Wanted, TARGETS_FILE, EXPORT_DIR):\n",
    "    #p = [f for f in os.listdir(DATA_DIR) if os.path.isfile(os.path.join(DATA_DIR, f))]\n",
    "    df = pd.read_csv(TARGETS_FILE)\n",
    "    df2 = pd.read_csv(Wanted)\n",
    "    df['filename'] = df['filename'].astype(str) + '.dcm'\n",
    "    new_df = df[~df['filename'].isin(df2['Name'])]\n",
    "    new_df_2 = df[~df['filename'].isin(new_df['filename'])]\n",
    "    new_df_2.to_csv(EXPORT_DIR,index=False)\n",
    "    #return new_df_2\n",
    "\n",
    "\n",
    "Wanted = 'C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/CSV/train_subtype_classes.csv'\n",
    "TARGETS_FILE = 'C:/Users/alqna/Documents/Bachelor/Bachelor_Data/csv/standard.csv'\n",
    "EXPORT_DIR ='C:/Users/alqna/Documents/Bachelor/rsna-intracranial-hemorrhage-detection/Big_Data/CSV/train_multilabel.csv'\n",
    "#a = create_multi_lable(Wanted,TARGETS_FILE,EXPORT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_window(dcm, window_center, window_width, U=1.0, eps=(1.0 / 255.0)):\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img = cp.array(np.array(img))\n",
    "    ue = cp.log((U / eps) - 1.0)\n",
    "    W = (2 / window_width) * ue\n",
    "    b = ((-2 * window_center) / window_width) * ue\n",
    "    z = W * img + b\n",
    "    img = U / (1 + cp.power(np.e, -1.0 * z))\n",
    "    img = (img - cp.min(img)) / (cp.max(img) - cp.min(img))\n",
    "    return cp.asnumpy(img)\n",
    "\n",
    "def sigmoid_bsb_window(img):\n",
    "    brain_img = sigmoid_window(img, 40, 80)\n",
    "    subdural_img = sigmoid_window(img, 80, 200)\n",
    "    bone_img = sigmoid_window(img, 600, 2000)\n",
    "    \n",
    "    bsb_img = np.zeros((3,brain_img.shape[0], brain_img.shape[1]))\n",
    "    bsb_img[0, :, :] = brain_img\n",
    "    bsb_img[1, :, :] = subdural_img\n",
    "    bsb_img[2, :, :] = bone_img\n",
    "    bsb_img = torch.tensor(bsb_img, dtype=torch.float32)\n",
    "    return bsb_img\n",
    "\n",
    "\n",
    "    \n",
    "def load_dicom(path):\n",
    "    img = pydicom.dcmread(path)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_data(inputs):\n",
    "    # normalization\n",
    "    #inputs = np.resize(inputs,(512,512))\n",
    "    \"\"\"\n",
    "    inputs = np.expand_dims(inputs, axis=0)\n",
    "    inputs = np.repeat(inputs,3,axis=0)\n",
    "    inputs -= inputs.mean()\n",
    "    inputs[inputs!=0] /= np.std(inputs[inputs!=0])\n",
    "    \"\"\"\n",
    "\n",
    "    #inputs = bsb_window(inputs)\n",
    "    inputs = sigmoid_bsb_window(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DicomDataset(Dataset):\n",
    "    def __init__(self ,rootPath, labelsPath, transform=None):\n",
    "        # load only all data paths\n",
    "        self.rootPath = rootPath\n",
    "        self.transform = transform\n",
    "        self.labels = pd.read_csv(labelsPath, index_col=0)\n",
    "        p = [f for f in os.listdir(rootPath) if os.path.isfile(os.path.join(rootPath, f))]\n",
    "        all_paths = np.array(p)\n",
    "        self.paths = all_paths\n",
    "        \n",
    "    def __len__(self):\n",
    "        # define size of dataset for dataloader\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # load data from path[item]\n",
    "        inputs = load_dicom(os.path.join(self.rootPath, self.paths[item-1]))\n",
    "        \n",
    "        outputs = {\n",
    "                'epidural':torch.tensor(self.labels.loc[self.paths[item-1]][\"epidural\"], dtype=torch.float32),\n",
    "                'intraparenchymal':torch.tensor(self.labels.loc[self.paths[item-1]][\"intraparenchymal\"], dtype=torch.float32),\n",
    "                'intraventricular':torch.tensor(self.labels.loc[self.paths[item-1]][\"intraventricular\"], dtype=torch.float32),\n",
    "                'subarachnoid':torch.tensor(self.labels.loc[self.paths[item-1]][\"subarachnoid\"], dtype=torch.float32),\n",
    "                'subdural':torch.tensor(self.labels.loc[self.paths[item-1]][\"subdural\"], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        inputs = preprocess_data(inputs)\n",
    "        if self.transform:\n",
    "            inputs = self.transform(inputs)\n",
    "        # return tuple\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_classification_model_resnet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet152(pretrained=True)\n",
    "        modules = list(self.model.children())[:-1]      \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        \n",
    "        self.classifier_BC = nn.Sequential(\n",
    "                          nn.Linear(2048, 512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.3),\n",
    "                          nn.Linear(512, 1),\n",
    "                          #nn.LogSoftmax(dim=1)\n",
    "                          )\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return {\n",
    "            'BC':  self.classifier_BC(x),\n",
    "\n",
    "        }\n",
    "    \n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        loss = F.binary_cross_entropy_with_logits(net_output['BC'], ground_truth['BC'].unsqueeze(1).to(device))\n",
    "      \n",
    "        return loss, {\n",
    "            'BC': Epidural_loss,\n",
    "        }\n",
    "    \n",
    "    def predict(self, images):\n",
    "        outputs = self(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_classification_model_resnet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet152(pretrained=True)\n",
    "        modules = list(self.model.children())[:-1]      \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        \n",
    "        self.classifier_Epidural = nn.Sequential(\n",
    "                          nn.Linear(2048, 512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.3),\n",
    "                          nn.Linear(512, 1),\n",
    "                          #nn.LogSoftmax(dim=1)\n",
    "                          )\n",
    "        self.classifier_Intraparenchymal = nn.Sequential(\n",
    "                          nn.Linear(2048, 512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.3),\n",
    "                          nn.Linear(512, 1),\n",
    "                          #nn.LogSoftmax(dim=1)\n",
    "                          )\n",
    "        self.classifier_Intraventricular = nn.Sequential(\n",
    "                          nn.Linear(2048, 512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.3),\n",
    "                          nn.Linear(512, 1),\n",
    "                          #nn.LogSoftmax(dim=1)\n",
    "                          )\n",
    "        self.classifier_Subarachnoid = nn.Sequential(\n",
    "                          nn.Linear(2048, 512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.3),\n",
    "                          nn.Linear(512, 1),\n",
    "                          #nn.LogSoftmax(dim=1)\n",
    "                          )\n",
    "        self.classifier_Subdural = nn.Sequential(\n",
    "                          nn.Linear(2048, 512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.3),\n",
    "                          nn.Linear(512, 1),\n",
    "                          #nn.LogSoftmax(dim=1)\n",
    "                          )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return {\n",
    "            'epidural':  self.classifier_Epidural(x),\n",
    "            'intraparenchymal': self.classifier_Intraparenchymal(x),\n",
    "            'intraventricular': self.classifier_Intraventricular(x),\n",
    "            'subarachnoid': self.classifier_Subarachnoid(x),\n",
    "            'subdural': self.classifier_Subdural(x)\n",
    "        }\n",
    "    \n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        Epidural_loss = F.binary_cross_entropy_with_logits(net_output['epidural'], ground_truth['epidural'].unsqueeze(1).to(device))\n",
    "        Intraparenchymal_loss = F.binary_cross_entropy_with_logits(net_output['intraparenchymal'], ground_truth['intraparenchymal'].unsqueeze(1).to(device))\n",
    "        Intraventricular_loss = F.binary_cross_entropy_with_logits(net_output['intraventricular'], ground_truth['intraventricular'].unsqueeze(1).to(device))\n",
    "        Subarachnoid_loss = F.binary_cross_entropy_with_logits(net_output['subarachnoid'], ground_truth['subarachnoid'].unsqueeze(1).to(device))\n",
    "        Subdural_loss = F.binary_cross_entropy_with_logits(net_output['subdural'], ground_truth['subdural'].unsqueeze(1).to(device))\n",
    "        loss = Epidural_loss + Intraparenchymal_loss + Intraventricular_loss + Subarachnoid_loss + Subdural_loss\n",
    "        return loss/5, {\n",
    "            'epidural': Epidural_loss,\n",
    "            'intraparenchymal': Intraparenchymal_loss,\n",
    "            'intraventricular': Intraventricular_loss,\n",
    "            'subarachnoid': Subarachnoid_loss,\n",
    "            'subdural': Subdural_loss\n",
    "        }\n",
    "    \n",
    "    def predict(self, images):\n",
    "        outputs = self(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ufoym/imbalanced-dataset-sampler/\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        self.num_samples = len(self.indices)  if num_samples is None else num_samples\n",
    "            \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            label_to_count[label] = label_to_count.get(label, 0) + 1\n",
    "            \"\"\"\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "            \"\"\"  \n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        \n",
    "        return dataset[idx][1].item()\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "        #return iter(torch.multinomial(self.weights, self.num_samples, replacement=True).tolist())\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    y_pred_tag = torch.ravel(y_pred_tag)\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(args, model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data)\n",
    "        loss_train, losses_train = model.get_loss(output, target)\n",
    "        total_loss += loss_train.item() * data.size(0)\n",
    "        loss_train.backward()\n",
    "        optimizer.step() \n",
    "              \n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    wandb.log({ # \"Test Accuracy\": 100. * correct / (len(test_loader)),\n",
    "    \"Train Loss\": total_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(args, model, device, test_loader, classes):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    example_images = []\n",
    "    total_acc = 0\n",
    "    acc_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model.forward(data)\n",
    "            loss_test, losses_test = model.get_loss(output, target)\n",
    "            total_loss += loss_test.item() * data.size(0)\n",
    "            \n",
    "            acc = 0\n",
    "            pred_list=''\n",
    "            truth_list=''\n",
    "            for label in classes:\n",
    "                acc_dict[label] = binary_acc(output[label].to(\"cpu\"), target[label])\n",
    "                acc += acc_dict[label]\n",
    "                \n",
    "                pred = torch.ravel(torch.round(torch.sigmoid(output[label])))\n",
    "                if int(pred[0].item()) == 1:\n",
    "                    #pred_list.append(label)\n",
    "                    pred_list+=label+'/'\n",
    "                    \n",
    "                if int(target[label][0].item()) == 1:\n",
    "                    #truth_list.append(label)\n",
    "                    truth_list+=label+'/'\n",
    "                #print(\"Pred: {} Truth: {}\".format(pred, target[label]))\n",
    "            #print(\"Pred: {} Truth: {}\".format(pred_list, truth_list))\n",
    "            example_images.append(wandb.Image(data[0],\n",
    "                                caption=\"Pred: {} Truth: {}\".format(pred_list, truth_list)))\n",
    "\n",
    "            acc/=5\n",
    "            total_acc += acc\n",
    "\n",
    "        del data, target, output\n",
    "        torch.cuda.empty_cache()\n",
    "        #print(\"acc \", total_acc/len(test_loader))\n",
    "        wandb.log({\n",
    "            \"Example Image\":example_images,\n",
    "            \"Test Loss\": total_loss/len(test_loader.dataset),\n",
    "            \"Test Accuracy\": total_acc/len(test_loader)}) \n",
    "        return total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, epoch, optimizer, loss, PATH):\n",
    "    checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'model': Multi_classification_model_resnet152()\n",
    "    }            \n",
    "    torch.save(checkpoint, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath): \n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#model = load_checkpoint('checkpoint.pth')\n",
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worldly-frog-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/alqnatri-ahmed/multi-lable-classification\" target=\"_blank\">https://wandb.ai/alqnatri-ahmed/multi-lable-classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/alqnatri-ahmed/multi-lable-classification/runs/2ggmoxs9\" target=\"_blank\">https://wandb.ai/alqnatri-ahmed/multi-lable-classification/runs/2ggmoxs9</a><br/>\n",
       "                Run data is saved locally in <code>/home/sheldon/Documents/Bachelor/wandb/run-20210425_131002-2ggmoxs9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"multi-lable-classification\")\n",
    "wandb.watch_called = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config          # Initialize config\n",
    "config.batch_size = 32         # input batch size for training (default: 64)\n",
    "config.test_batch_size = 32    # input batch size for testing (default: 1000)\n",
    "config.epochs = 50             # number of epochs to train (default: 10)\n",
    "config.lr = 0.01               # learning rate (default: 0.01)\n",
    "config.momentum = 0.9          # SGD momentum (default: 0.5) \n",
    "config.no_cuda = False         # disables CUDA training\n",
    "config.seed = 42               # random seed (default: 42)\n",
    "config.log_interval = 1     # how many batches to wait before logging training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58fc1f4f6184365a1dd43a59690768b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    print(\"device\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    torch.manual_seed(config.seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    " \n",
    "    \n",
    "    DATA_DIR_TRAIN = '../Final Bachelor/Data/MultiLabel_Data/TRAIN'\n",
    "    DATA_DIR_TEST = '../Final Bachelor/Data/MultiLabel_Data/TRAIN'\n",
    "    DATA_TARGET_TRAIN = '../Final Bachelor/Data/MultiLabel_Data/TRAIN/train_multilabel.csv'\n",
    "    DATA_TARGET_TEST = '../Final Bachelor/Data/MultiLabel_Data/TRAIN/test_multilabel.csv'\n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(265),\n",
    "        transforms.CenterCrop(224)\n",
    "    ])\n",
    "    \n",
    "    train = DicomDataset(rootPath=DATA_DIR_TRAIN, labelsPath=DATA_TARGET_TRAIN, transform=data_transforms)\n",
    "    test  = DicomDataset(rootPath=DATA_DIR_TEST, labelsPath=DATA_TARGET_TEST, transform=data_transforms)\n",
    "    \n",
    "    # data loader\n",
    "    train_loader = DataLoader(dataset=train, batch_size=config.batch_size, drop_last=True, num_workers = 0, pin_memory=True, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test, batch_size=config.test_batch_size, drop_last=True,  num_workers = 0, pin_memory=True, shuffle=True)\n",
    "\n",
    "    model = Multi_classification_model_resnet152().to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
    "    #optimizer = optim.Adam(model.fc.parameters(), lr=config.lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=2)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    PATH = '../Final Bachelor/Model/multi_label.pth'\n",
    "    classes = ('epidural','intraparenchymal','intraventricular','subarachnoid','subdural')\n",
    "    \n",
    "    for epoch in tqdm(range(1, config.epochs + 1)):\n",
    "        train_func(config, model, device, train_loader, optimizer)\n",
    "        loss_val = test_func(config, model, device, test_loader,classes)\n",
    "        scheduler.step(loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            save_checkpoint(model, epoch, optimizer, loss_val, PATH)\n",
    "    \n",
    "    wandb.save(os.path.join(wandb.run.dir, \"multi_label.pth\"))\n",
    "    \n",
    "    BC_model = load_checkpoint('../Final Bachelor/Model/binary.h5')\n",
    "    Multi_model = load_checkpoint('../Final Bachelor/Model/multi_label.pth')\n",
    "    image = '../Final Bachelor/Data/Binary_Data/TEST/ID_000c8cc79.dcm'\n",
    "    Binary_predicted = BC_model.predict(image)\n",
    "    if Binary_predicted == 1:\n",
    "        print(\"ICH Positive\")\n",
    "        Multi_predicted = Multi_model.predict(image)\n",
    "        ans = []\n",
    "        for label in classes:\n",
    "            if Multi_predicted[label] == 1:\n",
    "                ans.append(label)\n",
    "            print(ans)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
